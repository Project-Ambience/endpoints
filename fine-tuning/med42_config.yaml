
# 1. Model & Method
model_name_or_path: m42-health/Llama3-Med42-8B
stage: sft
finetuning_type: lora


# 2. LoRA settings
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1

# 3. Data
dataset: medical_finetune_test_data
dataset_dir: ./data/medical_finetune_test_data
val_size: 0.1
template: llama3
cutoff_len: 1024

# 4. Training hyperparameters
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 2e-5
num_train_epochs: 1
max_steps: 1
warmup_steps: 10

# 5. Checkpointing & evaluation
eval_strategy: steps
eval_steps: 50
logging_steps: 10
save_strategy: steps
save_steps: 50
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false

# 6. Output & precision
output_dir: ./outputs/med42_finetuned
overwrite_output_dir: true
fp16: true
gradient_checkpointing: true

# 7. Distributed
ddp_timeout: 180000000
report_to: []
