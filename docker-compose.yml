services:
  download_service:
    build: .
    image: services
    container_name: download_service
    command: ["./start_download.sh"]
    runtime: habana
    environment:
      - HABANA_VISIBLE_DEVICES=all
      - HF_HOME=/models
    network_mode: host
    ipc: host
    cap_add:
      - SYS_NICE
    volumes:
      - /shared/models:/models

  inference_service:
    build: .
    image: services
    container_name: inference_service
    command: ["./start_inference.sh"]
    runtime: habana
    environment:
      - HABANA_VISIBLE_DEVICES=7
      - HF_HOME=/models
      - PYTHONUNBUFFERED=1
    network_mode: host
    ipc: host
    cap_add:
      - SYS_NICE
    volumes:
      - /shared/models:/models

  prompteng_service:
    build: .
    image: services
    container_name: prompteng_service
    command: ["./start_prompteng.sh"]
    runtime: habana
    environment:
      - HABANA_VISIBLE_DEVICES=all
      - HF_HOME=/models
    network_mode: host
    ipc: host
    cap_add:
      - SYS_NICE
    volumes:
      - /shared/models:/models

  finetuning_service:
    build: .
    image: services
    container_name: finetuning_service
    command: ["./start_finetune.sh"]
    runtime: habana
    environment:
      - HABANA_VISIBLE_DEVICES=all
      - HF_HOME=/models
    network_mode: host
    ipc: host
    cap_add:
      - SYS_NICE
    volumes:
      - /shared/models:/models
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /shared/tmp:/tmp
  
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/llmedic.rules.yml:/etc/prometheus/llmedic.rules.yml
    ports:
      - "9090:9090"
    network_mode: host

  node_exporter:
    image: prom/node-exporter:latest
    container_name: node_exporter
    pid: "host"
    network_mode: host

